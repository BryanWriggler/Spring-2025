\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[margin = 2.54cm]{geometry}
\usepackage[most]{tcolorbox}

\newtcolorbox{myBox}[3]{
arc=5mm,
lower separated=false,
fonttitle=\bfseries,
%colbacktitle=green!10,
%coltitle=green!50!black,
enhanced,
attach boxed title to top left={xshift=0.5cm,
        yshift=-2mm},
colframe=blue!50!black,
colback=blue!10
}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\linespread{1.2}

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{question}{Question}

\title{Math 118C HW3}
\author{Zih-Yu Hsieh}

\begin{document}
\maketitle

\section*{1}
\begin{myBox}[]{}
    \begin{question}
        Rudin Pg. 241 Problem 19:

        Show that the system of equations
        $$3x+y-z+u^2=0$$
        $$x-y+2z+u=0$$
        $$2x+2y-3z+2u=0$$
        can be solved for $x,y,u$ in terms of $z$; for $x,z,u$ in terms of $y$; for $y,z,u$ in terms of $x$; but not for $x,y,z$ in terms of $u$.
    \end{question}
\end{myBox}

\textbf{Pf:}

\break

\section*{2}
\begin{myBox}[]{}
    \begin{question}
        Rudin Pg. 242 Problem 23:

        Define $f$ in $\mathbb{R}^3$ by
        $$f(x,y_1,y_2)=x^2y_1+e^x+y_2$$
        Show that $f(0,1,-1)=0$,$(D_1f)(0,1,-1)\neq 0$, and that there exists therefore a differentiable function $g$ in some neighborhood of $(1,-1)$ in $\mathbb{R}^2$,
        such that $g(1,-1)=0$ and 
        $$f(g(y_1,y_2),y_1,y_2)=0$$
        Find $(D_1g)(1,-1)$ and $(D_2g)(1,-1)$.
    \end{question}
\end{myBox}

\textbf{Pf:}

\textbf{$f$ and $Df$ at $(0,1,-1)$:}

Consider $f(0,1,-1)$, we get:
$$$$

\break

\section*{3}
\begin{myBox}[]{}
    \begin{question}
        Rudin Pg. 242 Problem 24:

        For $(x,y)\neq (0,0)$, define $f=(f_1,f_2)$ by 
        $$f_1(x,y)=\frac{x^2-y^2}{x^2+y^2},\quad f_2(x,y)=\frac{xy}{x^2+y^2}$$
        Compute the rank of $f'(x,y)$, and find the range of $f$.
    \end{question}
\end{myBox}

\textbf{Pf:}

\textbf{Rank of $f'(x,y)=Df(x,y)$:}

Given $f_1$ and $f_2$, their partial derivatives are given as follow:
$$\frac{\partial f_1}{\partial x} = \frac{2x(x^2+y^2)-2x(x^2-y^2)}{(x^2+y^2)^2} = \frac{4xy^2}{(x^2+y^2)^2},\quad \frac{\partial f_1}{\partial y}=\frac{-2y(x^2+y^2)-2y(x^2-y^2)}{(x^2+y^2)^2}=\frac{-4x^2y}{(x^2+y^2)^2}$$
$$\frac{\partial f_2}{\partial x}=\frac{y(x^2+y^2)-2x(xy)}{(x^2+y^2)^2}=\frac{y^3-x^2y}{(x^2+y^2)^2},\quad \frac{\partial f_2}{\partial y}=\frac{x(x^2+y^2)-2y(xy)}{(x^2+y^2)^2}=\frac{x^3-xy^2}{(x^2+y^2)^2}$$
Hence, the differential $Df$ is given as:
$$Df(x,y)=\begin{pmatrix}
    \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial y}\\
    \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial y}
\end{pmatrix}=\frac{1}{(x^2+y^2)^2}\begin{pmatrix}
    4xy^2 & -4x^2y\\
    (y^3-x^2y) & (x^3-xy^2)
\end{pmatrix}$$

\begin{itemize}
    \item If $x=0$, the matrix is given by:
    $$Df(0,y) = \frac{1}{y^4}\begin{pmatrix}
        0&0\\ y^3 & 0
    \end{pmatrix}$$
    Which, with $y\neq 0$ (due to the condition $(x,y)\neq (0,0)$), the above matrix has rank $1$.
    \item If $y=0$, the matrix is given by:
    $$Df(x,0)=\frac{1}{x^4}\begin{pmatrix}
        0&0\\0& x^3
    \end{pmatrix}$$
    Which again, with $x\neq 0$, the above matrix has rank $1$.
    \item If both $x,y\neq 0$, then consider any vector $r(x,-y)$ for $r\in\mathbb{R}$, we get::
    $$Df(x,y)r\begin{pmatrix}x\\-y\end{pmatrix} =\frac{1}{(x^2+y^2)^2}\begin{pmatrix}
        xy^2&-x^2y\\
        (y^3-x^2y) & (x^3-xy^2)
    \end{pmatrix}r\begin{pmatrix}x\\-y\end{pmatrix}$$
    $$\frac{r}{(x^2+y^2)^2}\begin{pmatrix}
        x^2y^2 - x^2y^2\\
        (xy^3-x^3y)-(-x^3y+xy^3)
    \end{pmatrix} = \bar{0}$$
    So the $\textmd{span}\{(x,-y)\}$ is within the null space of $Df(x,y)$, so the dimension of null space is at least $1$. On the other hand, consider $(1,0)\in\mathbb{R}^2$ (that is linearly independent with $(x,-y)$, since $(x,-y)$ has both entries being nonzero), it has the following:
    $$Df(x,y)\begin{pmatrix}1\\0\end{pmatrix} = \frac{1}{(x^2+y^2)^2}\begin{pmatrix}
        xy^2&-x^2y\\
        (y^3-x^2y) & (x^3-xy^2)
    \end{pmatrix}r\begin{pmatrix}1\\0\end{pmatrix} = \frac{1}{(x^2+y^2)^2}\begin{pmatrix}
        xy^2\\(y^3-x^2y)
    \end{pmatrix} \neq \bar{0}$$
    Which, it shows that the range of $Df(x,y)$ is nontrivial, hence it has dimension at least $1$ also.

    Because both the null space and the range have dimension $\geq 1$, while $\mathbb{R}^2$ has dimension $2$, by Rank Nullity Theorem, it enforces both the null space and the range must have dimension precisely $1$ (since the sum of the dimension of the null space and the range must be $2$).
    So, the rank of $Df(x,y)$ is again $1$.
\end{itemize}

So, regardless of the case, $Df(x,y)$ has rank $1$.

\hfil

\textbf{Range of $f$:}

When fixing $(x,y)\neq (0,0)$ in $\mathbb{R}^2$, there exists $r>0$, and $\theta\in [0,2\pi)$, such that $x=r\cos(\theta)$ and $y=r\sin(\theta)$ (under polar coordinates). Then, we get the output of $f_1,f_2$ as:
$$f_1(x,y)=\frac{(r\cos(\theta))^2-(r\sin(\theta))^2}{(r\cos(\theta))^2+(r\sin(\theta))^2}=\frac{r^2(\cos^2(\theta)-\sin^2(\theta))}{r^2} = \cos^2(\theta)-\sin^2(\theta) = \cos(2\theta)$$
$$f_2(x,y)=\frac{(r\cos(\theta))(r\sin(\theta))}{(r\cos(\theta))^2+(r\sin(\theta))^2} = \frac{r^2\cos(\theta)\sin(\theta)}{r^2} = \sin(\theta)\cos(\theta) = \frac{1}{2}\sin(2\theta)$$
Hence, for all $(x,y)\neq (0,0)$, $f=(f_1,f_2)$ satisfies the following equation:
$$f_1^2 + 4f_2^2 = \cos^2(2\theta) + 4\cdot \frac{1}{4}\sin^2(2\theta) = \cos^2(2\theta)+\sin^2(2\theta) = 1$$
Hence, $(u,v)=f(x,y)$ is a solution to $u^2+4v^2=1$, so the range of $f$ is contained in the ellipse characterized by $u^2+4v^2=1$.

On the other hand, for all point $(u,v)$ satisfying $u^2+4v^2=1$, there exists $\theta\in [0,2\pi)$, such that $u=\cos(\theta)$ and $v=\frac{1}{2}\sin(\theta)$. Then, consider the point $(\cos(\frac{\theta}{2}),\sin(\frac{\theta}{2}))\in\mathbb{R}^2$, we have:
$$f(\cos(\frac{\theta}{2}),\sin(\frac{\theta}{2})) = (f_1(\cos(\frac{\theta}{2}),\sin(\frac{\theta}{2})),f_2(\cos(\frac{\theta}{2}),\sin(\frac{\theta}{2}))) = \left(\cos\left(2\cdot\frac{\theta}{2}\right),\frac{1}{2}\sin\left(2\cdot\frac{\theta}{2}\right)\right)$$
$$ = \left(\cos(\theta),\frac{1}{2}\sin(\theta)\right) = (u,v)$$
Hence, $(u,v)$ is also in the range of $f$. This proves that $f$ has the range precisely described by the ellipse $u^2+4v^2=1$.

\break

\section*{4}
\begin{myBox}[]{}
    \begin{question}
        Rudin Pg. 242 Problem 25:

        Suppose $A\in\mathcal{L}(\mathbb{R}^n,\mathcal{R}^m)$, let $r$ be the rank of $A$.
        \begin{itemize}
            \item[(a)] Define $S$ as in the proof of Theorem $9.32$. Show that $SA$ is a projection in $\mathbb{R}^n$ whose null space is $\textmd{null}(A)$ and whose range is $\textmd{range}(S)$.
            \item[(b)] Use (a) to show that 
            $$\dim(\textmd{null}(A))+\dim(\textmd{range}(A))=n$$
        \end{itemize}
    \end{question}
\end{myBox}

\textbf{Pf:}
\begin{itemize}
    \item[(a)] Given that $A$ has rank $r$, then its range $\textmd{range}(A)\subseteq \mathbb{R}^m$ is an $r$-dimensional linear subspace, hence there exists $y_1,...,y_r\in \textmd{range}(A)$ that forms a basis of it.
    
    Then, by the text in Rudin, choose $z_1,...,z_r\in\mathbb{R}^n$, so for each index $i\in \{1,...,r\}$, $Az_i=y_i$. Which, the collection $z_1,...,z_r\in\mathbb{R}^n$ is linearly independent, since if $a_1,...,a_r\in\mathbb{R}$ satisfies $\sum_{i=1}^{r}a_iz_i = \bar{0}$, then the following is true:
    $$A\left(\sum_{i=1}^{r}a_iz_i\right) = \sum_{i=1}^{r}a_i(Az_i) = \sum_{i=1}^{r}a_iy_i$$
    By the linear independence of $y_1,...,y_r\in \textmd{range}(A)$, each $a_i = 0$, which proves the linear independence of $z_1,...,z_r\in\mathbb{R}^n$.

    \begin{comment}
    Now, we can expand the linearly independent list $z_1,...,z_r\in\mathbb{R}^n$ into a basis, say $z_1,...,z_r,x_1,...,x_k\in\mathbb{R}^n$ with $r+k=n$.
    Which, each $x_j$ with $j\in\{1,...,k\}$ must satisfy $Ax_j = \bar{0}\in\mathbb{R}^m$ (or $x_j\in \textmd{null}(A)\subseteq\mathbb{R}^n$): Since $Ax_j\in\textmd{range}(A)$, then it is spanned by $y_1,...,y_r$, there exists $a_1,...,a_r\in\mathbb{R}$ such that the following equation is true:
    $$Ax_j = \sum_{i=1}^{r}a_iy_i$$
    By the definition of $z_i$ given above, $Ax_j$ can then be rewrite as:
    $$Ax_j = \sum_{i=1}^{r}a_iy_i = \sum_{i=1}^{r}a_i(Az_i) = A\left(\sum_{i=1}^{r}a_iz_i\right)$$
    \end{comment}

    Now, define $S\in\mathcal{L}(\textmd{range}(A),\mathbb{R}^n)$ the same as in the text, which has the following formula:
    $$\forall c_1,...,c_r\in\mathbb{R},\quad S\left(\sum_{i=1}^{r}c_iy_i\right)=\sum_{i=1}^{r}c_iz_i$$
    Then, for all $x\in\mathbb{R}^n$, since $Ax\in\textmd{range}(A)$, it is spanned by $y_1,...,y_r$, hence there exists unique $a_1,...,a_r\in\mathbb{R}$, such that the following is true:
    $$Ax = \sum_{i=1}^{r}a_iy_i$$
    Hence, we get the following:
    $$SAx = S\left(\sum_{i=1}^{r}a_iy_i\right) = \sum_{i=1}^{r}a_iz_i$$
    Hence, applying $SA$ twice, we get:
    $$SA(SAx) = SA\left(\sum_{i=1}^{r}a_iz_i\right) = S\left(\sum_{i=1}^{r}a_iy_i\right) = \sum_{i=1}^{r}a_iz_i$$
    This shows that $SA(SAx) = SAx$ for all $x\in\mathbb{R}^n$, hence $SA$ is a projection on $\mathbb{R}^n$.

    \hfil

    Now, to find the null space and range, consider the following:
    \begin{itemize}
        \item For all $x\in\textmd{null}(A)$, since $Ax = 0$, then $SAx = S(0)=0$, so $x\in \textmd{null}(SA)$, or $\textmd{null}(A)\subseteq \textmd{null}(SA)$.
        
        On the other hand, for all $\in \textmd{null}(SA)$, since $S(Ax)=0$, $Ax \in \textmd{null(S)}$. But, since $Ax\in\textmd{range}(A)$, there exists unique $a_1,...,a_r\in\mathbb{R}$, with $Ax = \sum_{i=1}^{r}a_iy_i$. Hence, we have the following:
        $$ 0 = S(Ax)=S\left(\sum_{i=1}^{r}a_iy_i\right) = \sum_{i=1}^{r}a_iz_i$$
        Hence, by linear independence of $z_1,...,z_r\in\mathbb{R}^n$, we must have $a_i=0$ for all index $i\in\{1,...,r\}$.
        This proves that $Ax = \sum_{i=1}^{r}a_iy_i = 0$, so $x\in\textmd{null}(A)$. Hence, $\textmd{null}(SA)\subseteq \textmd{null}(A)$, showing that $\textmd{null}(SA)= \textmd{null}(A)$.

        \item For all $z\in\textmd{range}(SA)$, there exists $x\in\mathbb{R}^n$ with $SAx = z$. Since $z=S(Ax)\in \textmd{range}(S)$, then $\textmd{range}(SA)\subseteq\textmd{range}(S)$.
        
        Similarly, for all $z\in\textmd{range}(S)$, there exists $y\in \textmd{range}(A)$ (the domain of $S$), with $Sy = z$; then because there exists $x\in\mathbb{R}^n$, with $Ax = y$ by the definition of range, we have $SAx = S(Ax) = Sy = z$, hence $z\in \textmd{range}(SA)$, proving that $\textmd{range}(S)\subseteq \textmd{range}(SA)$, or $\textmd{range}(S)=\textmd{range}(SA)$.
    \end{itemize}
    Hence, the above to cases proves that $\textmd{null}(SA)= \textmd{null}(A)$, while $\textmd{range}(S)=\textmd{range}(SA)$. So, $SA$ is a projection in $\mathbb{R}^n$ with null space being $\textmd{null}(A)$, and range being $\textmd{range}(S)$.

    \hfil

    \item[(b)] With the linearly independent set $z_1,...,z_r\in\mathbb{R}^n$ given in \textbf{part (a)}, we'll consider an extra list $x_1,...,x_k\in \textmd{null}(A)\subseteq\mathbb{R}^n$ that forms a basis of $\textmd{null}(A)$. Our goal is to prove that $x_1,...,x_k,z_1,...,z_r$ forms a basis of $\mathbb{R}^n$.
    
    First, consider $a_1,...,a_k,b_1,...,b_r\in\mathbb{R}$, suppose the vector $\sum_{i=1}^{k}a_ix_i+\sum_{j=1}^{r}b_jz_j = \bar{0}\in\mathbb{R}^n$, then we have the following:
    $$0=A(\bar{0})=A\left(\sum_{i=1}^{k}a_ix_i+\sum_{j=1}^{r}b_jz_j\right) = A\left(\sum_{i=1}^{k}a_ix_i\right)+A\left(\sum_{j=1}^{r}b_jz_j\right)=\sum_{j=1}^{r}b_j(Az_j) = \sum_{j=1}^{r}b_jy_j$$
    Which, by the linear independence of $y_1,...,y_r\in\textmd{range}(A)$ assumed in \textbf{part (a)}, we must have $b_j = 0$ for all $j\in\{1,...,n\}$. So, $\sum_{i=1}^{k}a_ix_i+\sum_{j=1}^{r}b_jz_j = \sum_{i=1}^{k}a_ix_i = \bar{0}$. But again, based on the linear independence ov $x_1,...,x_k$ by assumption, we get $a_i=0$ for all $i\in\{1,...,k\}$.
    This proves that all $a_i,b_j = 0$, which the collection $x_1,...,x_k,z_1,...,z_r$ is linearly independent.

    Then, for all $x\in\mathbb{R}^n$, since $Ax \in\textmd{range}(A)$, then there exists unique $b_1,...,b_r\in\mathbb{R}$, with $Ax =\sum_{j=1}^{r}b_jy_j$. Hence, we get the following:
    $$Ax=\sum_{j=1}^{r}b_jy_j=\sum_{j=1}^{r}b_j(Az_j) = A\left(\sum_{j=1}^{r}b_jz_j\right)$$
    So, we can reduce to the following:
    $$Ax-A\left(\sum_{j=1}^{r}b_jz_j\right)=A\left(x-\sum_{j=1}^{r}b_jz_j\right) = 0,\quad x-\sum_{j=1}^{r}b_jz_j\in\textmd{null}(A)$$
    Then, since $x_1,...,x_k$ forms a basis of $\textmd{null}(A)$, then there exists unique $a_1,...,a_k\in\mathbb{R}$, with:
    $$x-\sum_{j=1}^{r}b_jz_j = \sum_{i=1}^{k}a_ix_i,\quad x=\sum_{i=1}^{k}a_ix_i+\sum_{j=1}^{r}b_jz_j$$
    So, $x\in \textmd{span}\{x_1,...,x_k,z_1,...,z_r\}$, proving that $\mathbb{R}^n = \textmd{span}\{x_1,...,x_k,z_1,...,z_r\}$.

    \hfil

    Hence, since $x_1,...,x_k,z_1,...,z_r$ spans $\mathbb{R}^n$ while being linearly independent, it is a basis of $\mathbb{R}^n$. Hence, the length of the basis, $k+r = \dim(\mathbb{R}^n) = n$.

    Which, since $x_1,...,x_k$ is a basis of $\textmd{null}(A)$, then $\dim(\textmd{null}(A)) = k$.
    
    On the other hand, since $S\in\mathcal{L}(\textmd{range}(A),\mathbb{R}^n)$ is injective (since it maps $y_1,...,y_r$ a basis of $\textmd{range}(A)$, to $z_1,...,z_r\in\mathbb{R}^n$ a linearly independent set), then the domain $\textmd{range}(A)$ and range are in fact isomorphic as vector spaces, while the range of $S$ is precisely $\textmd{span}\{z_1,...,z_r\}$ (since the definition of $S$ is maps $y_i$ to $z_i$ for each $i\in\{1,...,r\}$, showing that the output value must be a linear combination of all $z_i$).
    Hence, $\dim(\textmd{range}(A)) = \dim(\textmd{span}\{z_1,...,z_r\}) = r$ (since $z_1,...,z_r$ is linearly independent, it forms a basis of the span).

    So, compile the information from above, we get:
    $$k+r = n,\quad k=\dim(\textmd{null}(A)),\quad r=\dim(\textmd{range}(A))$$
    $$\implies \dim(\textmd{null}(A))+\dim(\textmd{range}(A))=n$$

\end{itemize}

\hfil

\hfil

\section*{5}
\begin{myBox}[]{}
    \begin{question}
        Rudin Pg. 242 Problem 26:

        Show that the existence (and even the continuity) of $D_{12}f$ does not imlpy the existence of $D_1f$. For example, let $f(x,y)=g(x)$, where $g$ is nowhere differentiable.
    \end{question}
\end{myBox}

\textbf{Pf:}

Consider the Weierstrass Functon $g:\mathbb{R}\rightarrow\mathbb{R}$, which is uniformly continuous, while being differentiable nowhere.

Then, given the function $f:\mathbb{R}^2\rightarrow\mathbb{R}$ by $f(x,y)=g(x)$, since $g$ is not differentiable with respect to its variable $x$, then $D_1f$ does not exist; yet, since $D_2f \equiv 0$ (due to the fact that $g$ is a constant when $x$ is fixed), then $D_{12}f = D_1(D_2f) = 0$.

Hence, even though $D_{12}f$ is continuous, $D_1f$ doesn't exist in this case.

\end{document}